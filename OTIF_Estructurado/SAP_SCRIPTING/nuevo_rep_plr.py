import win32com.client
from datetime import datetime
import os
import pandas as pd
import time
import json

# Check if the current time is after 2 PM
current_hour = datetime.now().hour
if current_hour >= 14:
    today_date_sap_format = datetime.today().strftime('%Y%m%d')
    
    # File path and name
    file_name = "REP_PLR_HOY.xls"
    saved_path = os.path.join(os.environ["USERPROFILE"], "Documents", file_name)
    
    # Remove file if it already exists
    if os.path.exists(saved_path):
        os.remove(saved_path)
    
    # Connect to SAP GUI
    try:
        sap_gui_auto = win32com.client.GetObject("SAPGUI")
        application = sap_gui_auto.GetScriptingEngine
        connection = application.Children(0)
        session = connection.Children(0)
    except Exception as e:
        print(f"Error connecting to SAP: {e}")
        exit()
        
    # SAP automation
    try:
        session.findById("wnd[0]").maximize()
        session.findById("wnd[0]/tbar[0]/okcd").text = "zsd_rep_planeamiento"
        session.findById("wnd[0]").sendVKey(0)
        
        session.findById("wnd[0]/tbar[1]/btn[17]").press()
        session.findById("wnd[1]/usr/txtENAME-LOW").text = ""
        session.findById("wnd[1]/usr/txtENAME-LOW").setFocus()
        session.findById("wnd[1]/usr/txtENAME-LOW").caretPosition = 0
        session.findById("wnd[1]/tbar[0]/btn[8]").press()
        
        session.findById("wnd[1]/usr/cntlALV_CONTAINER_1/shellcont/shell").currentCellRow = 11
        session.findById("wnd[1]/usr/cntlALV_CONTAINER_1/shellcont/shell").selectedRows = "11"
        session.findById("wnd[1]/usr/cntlALV_CONTAINER_1/shellcont/shell").doubleClickCurrentCell()
        
        session.findById("wnd[0]/usr/ctxtP_LFDAT-LOW").setFocus()
        session.findById("wnd[0]/usr/ctxtP_LFDAT-LOW").caretPosition = 1
        session.findById("wnd[0]").sendVKey(4)
        session.findById("wnd[1]/usr/cntlCONTAINER/shellcont/shell").focusDate = today_date_sap_format
        session.findById("wnd[1]/usr/cntlCONTAINER/shellcont/shell").selectionInterval = f"{today_date_sap_format},{today_date_sap_format}"
        
        session.findById("wnd[0]/tbar[1]/btn[8]").press()
        
        # Wait for the report to generate
        time.sleep(5)
        
        session.findById("wnd[0]/mbar/menu[0]/menu[3]/menu[2]").select()
        session.findById("wnd[1]/usr/subSUBSCREEN_STEPLOOP:SAPLSPO5:0150/sub:SAPLSPO5:0150/radSPOPLI-SELFLAG[1,0]").select()
        session.findById("wnd[1]/usr/subSUBSCREEN_STEPLOOP:SAPLSPO5:0150/sub:SAPLSPO5:0150/radSPOPLI-SELFLAG[1,0]").setFocus()
        session.findById("wnd[1]/tbar[0]/btn[0]").press()
        
        session.findById("wnd[1]/usr/ctxtDY_PATH").text = os.path.dirname(saved_path)
        session.findById("wnd[1]/usr/ctxtDY_PATH").setFocus()
        session.findById("wnd[1]/usr/ctxtDY_PATH").caretPosition = 0
        session.findById("wnd[1]").sendVKey(4)
        session.findById("wnd[2]/usr/ctxtDY_FILENAME").text = file_name
        session.findById("wnd[2]/usr/ctxtDY_FILE_ENCODING").text = "0000"
        session.findById("wnd[2]/usr/ctxtDY_FILENAME").caretPosition = len(file_name)
        session.findById("wnd[2]/tbar[0]/btn[0]").press()
        session.findById("wnd[1]/tbar[0]/btn[0]").press()
        
        print("Script executed correctly with today's date. File saved in:", saved_path)
        
        # Wait for file to be completely written
        time.sleep(3)
        
    except Exception as e:
        print(f"Error during SAP automation: {e}")
        exit()

    # --- Post-processing to create Power BI compatible files ---
    try:
        # Define file names and paths for Power BI compatible files
        base_name = "REP_PLR_HOY"
        
        # Use the specified directory C:\Data\Nite
        data_dir = r"C:\Data\Nite"
        
        # Ensure data directory exists
        os.makedirs(data_dir, exist_ok=True)
        print(f"üìÅ Using output directory: {data_dir}")
        
        # Power BI compatible file paths in the data folder
        excel_path = os.path.join(data_dir, f"{base_name}_PowerBI.xlsx")
        csv_path = os.path.join(data_dir, f"{base_name}_PowerBI.csv")
        parquet_path = os.path.join(data_dir, f"{base_name}_PowerBI.parquet")
        
        # Read the HTML file generated by SAP (disguised as XLS)
        # Try different encodings to handle SAP file encoding issues
        content = None
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
        
        for encoding in encodings_to_try:
            try:
                with open(saved_path, 'r', encoding=encoding) as f:
                    content = f.read()
                print(f"Successfully read file with encoding: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            print("‚ùå Could not read file with any of the attempted encodings")
            return
        
        # Check if it's actually HTML (common SAP behavior)
        if content.strip().startswith('<'):
            print("Detected HTML file with XLS extension - converting for Power BI...")
            # Try to read HTML tables with different encodings
            tables = None
            for encoding in encodings_to_try:
                try:
                    tables = pd.read_html(saved_path, encoding=encoding)
                    print(f"Successfully read HTML tables with encoding: {encoding}")
                    break
                except:
                    continue
            
            if tables is None:
                print("‚ùå Could not read HTML tables with any encoding")
                return
            
            # Assuming the first table is the one we need
            if tables:
                df = tables[0]
                
                # Clean up the DataFrame - remove any entirely empty rows and columns
                df.dropna(how='all', inplace=True)
                df.dropna(axis=1, how='all', inplace=True)
                
                # Reset index after dropping rows
                df.reset_index(drop=True, inplace=True)
                
                # Power BI Data Transformation and Cleaning
                df = transform_data_for_powerbi(df)
                
                # Save in multiple formats for Power BI compatibility
                save_powerbi_files(df, excel_path, csv_path, parquet_path)
                
            else:
                print("No tables found in the HTML file")
        else:
            # Use specialized SAP file processing (proven to work)
            print("üìä Processing as specialized SAP file...")
            df = process_sap_file_content(saved_path, encodings_to_try)
            
            if df is None:
                print("‚ùå Could not process SAP file")
                return
                
            df = transform_data_for_powerbi(df)
            save_powerbi_files(df, excel_path, csv_path, parquet_path)
            
    except Exception as e:
        print(f"Error during Power BI file creation: {e}")
        
else:
    print("The current time is before 2 PM. The script is not running.")


def process_sap_file_content(file_path, encodings_to_try):
    """
    Process SAP file with specialized method that works
    """
    try:
        # Read the file line by line to understand its structure
        lines = None
        for encoding in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    lines = f.readlines()
                print(f"Successfully read file with encoding: {encoding}")
                break
            except:
                continue
        
        if lines is None:
            print("‚ùå Could not read file with any encoding")
            return None
        
        print(f"üìä File has {len(lines)} lines")
        
        # Find the data section (skip headers)
        data_start = None
        for i, line in enumerate(lines):
            if 'Centro' in line and 'Fe.Entrega' in line and 'Ruta' in line:
                data_start = i
                print(f"üìã Found header at line {i+1}")
                break
        
        if data_start is None:
            print("‚ùå Could not find data header")
            return None
        
        # Extract header
        header_line = lines[data_start].strip()
        headers = [col.strip() for col in header_line.split('\t')]
        print(f"üìã Found {len(headers)} columns: {headers[:5]}...")
        
        # Extract data rows
        data_rows = []
        for i in range(data_start + 1, len(lines)):
            line = lines[i].strip()
            if line and not line.startswith('16.09.2025'):  # Skip date headers
                row_data = line.split('\t')
                if len(row_data) >= len(headers):
                    data_rows.append(row_data[:len(headers)])
        
        print(f"üìä Found {len(data_rows)} data rows")
        
        # Create DataFrame
        df = pd.DataFrame(data_rows, columns=headers)
        
        # Clean up the DataFrame
        df.dropna(how='all', inplace=True)
        df.dropna(axis=1, how='all', inplace=True)
        df.reset_index(drop=True, inplace=True)
        
        print(f"‚úÖ DataFrame created with shape: {df.shape}")
        return df
        
    except Exception as e:
        print(f"‚ùå Error processing SAP file content: {e}")
        return None


def transform_data_for_powerbi(df):
    """
    Transform and clean data for optimal Power BI compatibility
    """
    print("üîÑ Transforming data for Power BI compatibility...")
    
    # Create a copy to avoid modifying the original
    df_clean = df.copy()
    
    # Remove any rows that are completely empty or contain only whitespace
    df_clean = df_clean.dropna(how='all')
    df_clean = df_clean[~df_clean.astype(str).eq('').all(axis=1)]
    
    # Clean column names - remove extra spaces and special characters
    df_clean.columns = df_clean.columns.str.strip()
    df_clean.columns = df_clean.columns.str.replace(r'\s+', ' ', regex=True)
    
    # Convert date columns to proper datetime format
    date_columns = ['Fe.Entrega', 'Fecha Gu√≠a', 'Creado el']
    for col in date_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to datetime")
    
    # Convert time columns to proper time format
    time_columns = ['Hora Gu√≠a', 'Hora']
    for col in time_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_datetime(df_clean[col], format='%H:%M:%S', errors='coerce').dt.time
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to time")
    
    # Convert numeric columns
    numeric_columns = ['Cajas R.S.', 'Cajas F√≠sicas', 'Cajas Equiv.', 'Ruta', 'Entrega', 'Cliente']
    for col in numeric_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to numeric")
    
    # Clean text columns - remove extra spaces
    text_columns = ['Centro', 'Nombre del Cliente', 'Estatus', 'Ruta Dist.', 'Cami√≥n', 
                   'Guia Entrega', 'Rol Alisto', 'Reprogramado', 'Ped. Clte', 'Ped. SAP',
                   'Tipo Ped.', 'Desc. Tipo', 'Origen', 'Tipo Ruta', 'Fuerza Ventas',
                   'Region', 'Zona Vtas', 'Desc Zona Vtas', 'Zona Superv', 'Desc Zona Superv',
                   'Agente Vtas', 'Nombre']
    
    for col in text_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str).str.strip()
            df_clean[col] = df_clean[col].replace('nan', '')
    
    # Add calculated columns useful for Power BI analysis
    if 'Fe.Entrega' in df_clean.columns:
        df_clean['A√±o'] = df_clean['Fe.Entrega'].dt.year
        df_clean['Mes'] = df_clean['Fe.Entrega'].dt.month
        df_clean['D√≠a'] = df_clean['Fe.Entrega'].dt.day
        df_clean['D√≠a_Semana'] = df_clean['Fe.Entrega'].dt.day_name()
        df_clean['Semana'] = df_clean['Fe.Entrega'].dt.isocalendar().week
    
    # Add performance indicators
    if 'Cajas R.S.' in df_clean.columns and 'Cajas F√≠sicas' in df_clean.columns:
        df_clean['Diferencia_Cajas'] = df_clean['Cajas F√≠sicas'] - df_clean['Cajas R.S.']
        df_clean['Porcentaje_Cumplimiento'] = (df_clean['Cajas F√≠sicas'] / df_clean['Cajas R.S.'] * 100).round(2)
    
    # Add status categories for better filtering in Power BI
    if 'Estatus' in df_clean.columns:
        df_clean['Estatus_Categoria'] = df_clean['Estatus'].apply(categorize_status)
    
    # Add route type categories
    if 'Tipo Ruta' in df_clean.columns:
        df_clean['Tipo_Ruta_Categoria'] = df_clean['Tipo Ruta'].apply(categorize_route_type)
    
    print(f"‚úÖ Data transformation completed. Final dataset shape: {df_clean.shape}")
    return df_clean


def categorize_status(status):
    """Categorize delivery status for better Power BI filtering"""
    if pd.isna(status):
        return 'Sin Estatus'
    
    status_str = str(status).strip()
    if status_str in ['5']:
        return 'Entregado'
    elif status_str in ['1', '2', '3']:
        return 'En Proceso'
    elif status_str in ['4']:
        return 'Pendiente'
    else:
        return 'Otro'


def categorize_route_type(route_type):
    """Categorize route type for better Power BI analysis"""
    if pd.isna(route_type):
        return 'Sin Tipo'
    
    route_str = str(route_type).strip().upper()
    if 'MODERNO' in route_str:
        return 'Moderno'
    elif 'TRADICIONAL' in route_str:
        return 'Tradicional'
    elif 'RURAL' in route_str:
        return 'Rural'
    else:
        return 'Otro'


def save_powerbi_files(df, excel_path, csv_path, parquet_path):
    """
    Save the dataframe in multiple formats optimized for Power BI
    """
    print("üíæ Saving files in Power BI compatible formats...")
    
    try:
        # Save as Excel with proper formatting
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='REP_PLR_Data', index=False)
            
            # Get the workbook and worksheet
            workbook = writer.book
            worksheet = writer.sheets['REP_PLR_Data']
            
            # Auto-adjust column widths
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"‚úÖ Excel file saved: {excel_path}")
        
        # Save as CSV with UTF-8 encoding (Power BI preferred)
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"‚úÖ CSV file saved: {csv_path}")
        
        # Save as Parquet for better performance in Power BI
        df.to_parquet(parquet_path, index=False)
        print(f"‚úÖ Parquet file saved: {parquet_path}")
        
        # Create a metadata file for Power BI
        create_powerbi_metadata(df, os.path.dirname(excel_path))
        
        print("\nüéâ All Power BI compatible files created successfully!")
        print("üìä Recommended file for Power BI: Use the .parquet file for best performance")
        print("üìã Use the .csv file if you need to import into other tools")
        print("üìà Use the .xlsx file for manual review and analysis")
        
    except Exception as e:
        print(f"‚ùå Error saving Power BI files: {e}")


def create_powerbi_metadata(df, output_path):
    """
    Create a metadata file with column descriptions for Power BI
    """
    try:
        metadata = {
            "dataset_info": {
                "name": "REP_PLR_HOY",
                "description": "Reporte de Planeamiento de Rutas - Datos del d√≠a",
                "created_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_records": len(df),
                "total_columns": len(df.columns)
            },
            "columns": {}
        }
        
        # Define column descriptions
        column_descriptions = {
            "Centro": "Centro de distribuci√≥n",
            "Fe.Entrega": "Fecha de entrega programada",
            "Ruta": "N√∫mero de ruta",
            "Entrega": "N√∫mero de entrega",
            "Cliente": "C√≥digo del cliente",
            "Nombre del Cliente": "Nombre del cliente",
            "Estatus": "Estatus de la entrega (5=Entregado)",
            "Verificado": "Indicador de verificaci√≥n",
            "Cajas R.S.": "Cajas requeridas seg√∫n sistema",
            "Cajas F√≠sicas": "Cajas f√≠sicas entregadas",
            "Cajas Equiv.": "Cajas equivalentes",
            "Ruta Dist.": "Ruta de distribuci√≥n",
            "Viaje": "N√∫mero de viaje",
            "Cami√≥n": "N√∫mero de cami√≥n",
            "Guia Entrega": "N√∫mero de gu√≠a de entrega",
            "Fecha Gu√≠a": "Fecha de la gu√≠a",
            "Hora Gu√≠a": "Hora de la gu√≠a",
            "Creado el": "Fecha de creaci√≥n del registro",
            "Hora": "Hora de creaci√≥n",
            "Rol Alisto": "Rol de alistamiento",
            "Reprogramado": "Indicador de reprogramaci√≥n",
            "Ped. Clte": "Pedido del cliente",
            "Ped. SAP": "Pedido en SAP",
            "Ped. Traslado": "Pedido de traslado",
            "Tipo Ped.": "Tipo de pedido",
            "Desc. Tipo": "Descripci√≥n del tipo",
            "Origen": "Origen del pedido",
            "Tipo Ruta": "Tipo de ruta",
            "Fuerza Ventas": "Fuerza de ventas",
            "Region": "Regi√≥n",
            "Zona Vtas": "Zona de ventas",
            "Desc Zona Vtas": "Descripci√≥n de zona de ventas",
            "Zona Superv": "Zona de supervisi√≥n",
            "Desc Zona Superv": "Descripci√≥n de zona de supervisi√≥n",
            "Agente Vtas": "C√≥digo del agente de ventas",
            "Nombre": "Nombre del agente de ventas",
            "A√±o": "A√±o de la entrega (calculado)",
            "Mes": "Mes de la entrega (calculado)",
            "D√≠a": "D√≠a de la entrega (calculado)",
            "D√≠a_Semana": "D√≠a de la semana (calculado)",
            "Semana": "Semana del a√±o (calculado)",
            "Diferencia_Cajas": "Diferencia entre cajas f√≠sicas y requeridas (calculado)",
            "Porcentaje_Cumplimiento": "Porcentaje de cumplimiento (calculado)",
            "Estatus_Categoria": "Categor√≠a del estatus (calculado)",
            "Tipo_Ruta_Categoria": "Categor√≠a del tipo de ruta (calculado)"
        }
        
        # Add column information
        for col in df.columns:
            metadata["columns"][col] = {
                "description": column_descriptions.get(col, "Sin descripci√≥n disponible"),
                "data_type": str(df[col].dtype),
                "null_count": int(df[col].isnull().sum()),
                "unique_values": int(df[col].nunique())
            }
        
        # Save metadata as JSON
        metadata_path = os.path.join(output_path, "REP_PLR_HOY_Metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Metadata file saved: {metadata_path}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not create metadata file: {e}")


def process_existing_file():
    """
    Process the existing REP_PLR_HOY.xls file in the data folder
    This function can be called independently to convert existing files
    """
    try:
        # Define file paths
        script_dir = os.path.dirname(os.path.abspath(__file__))
        source_data_dir = os.path.join(script_dir, "data")
        existing_file = os.path.join(source_data_dir, "REP_PLR_HOY.xls")
        
        # Use the specified directory C:\Data\Nite for output
        data_dir = r"C:\Data\Nite"
        os.makedirs(data_dir, exist_ok=True)
        print(f"üìÅ Using output directory: {data_dir}")
        
        # Check if the existing file exists
        if not os.path.exists(existing_file):
            print(f"‚ùå File not found: {existing_file}")
            return False
        
        print(f"üìÅ Processing existing file: {existing_file}")
        
        # Define output file paths
        base_name = "REP_PLR_HOY"
        excel_path = os.path.join(data_dir, f"{base_name}_PowerBI.xlsx")
        csv_path = os.path.join(data_dir, f"{base_name}_PowerBI.csv")
        parquet_path = os.path.join(data_dir, f"{base_name}_PowerBI.parquet")
        
        # Read the file - try different encodings to handle SAP file encoding issues
        content = None
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
        
        for encoding in encodings_to_try:
            try:
                with open(existing_file, 'r', encoding=encoding) as f:
                    content = f.read()
                print(f"Successfully read file with encoding: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            print("‚ùå Could not read file with any of the attempted encodings")
            return False
        
        # Check if it's HTML (common SAP behavior)
        if content.strip().startswith('<'):
            print("üîç Detected HTML file with XLS extension - converting for Power BI...")
            # Try to read HTML tables with different encodings
            tables = None
            for encoding in encodings_to_try:
                try:
                    tables = pd.read_html(existing_file, encoding=encoding)
                    print(f"Successfully read HTML tables with encoding: {encoding}")
                    break
                except:
                    continue
            
            if tables is None:
                print("‚ùå Could not read HTML tables with any encoding")
                return False
            
            if tables:
                df = tables[0]
                
                # Clean up the DataFrame
                df.dropna(how='all', inplace=True)
                df.dropna(axis=1, how='all', inplace=True)
                df.reset_index(drop=True, inplace=True)
                
                # Transform for Power BI
                df = transform_data_for_powerbi(df)
                
                # Save in multiple formats
                save_powerbi_files(df, excel_path, csv_path, parquet_path)
                
                print("‚úÖ Existing file processed successfully!")
                return True
            else:
                print("‚ùå No tables found in the HTML file")
                return False
        else:
            # If it's a real Excel file
            print("üìä Processing as Excel file...")
            df = pd.read_excel(existing_file)
            df = transform_data_for_powerbi(df)
            save_powerbi_files(df, excel_path, csv_path, parquet_path)
            
            print("‚úÖ Existing file processed successfully!")
            return True
            
    except Exception as e:
        print(f"‚ùå Error processing existing file: {e}")
        return False


# Uncomment the line below to process the existing file when running the script
# process_existing_file()