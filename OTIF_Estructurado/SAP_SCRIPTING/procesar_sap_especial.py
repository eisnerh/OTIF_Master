#!/usr/bin/env python3
"""
Script especializado para procesar archivos SAP con formato espec√≠fico
"""

import os
import pandas as pd
import json
from datetime import datetime

def process_sap_file():
    """
    Procesa el archivo SAP con formato espec√≠fico
    """
    try:
        # Define file paths
        script_dir = os.path.dirname(os.path.abspath(__file__))
        source_data_dir = os.path.join(script_dir, "data")
        existing_file = os.path.join(source_data_dir, "REP_PLR_HOY.xls")
        
        # Use the specified directory C:\Data\Nite for output
        data_dir = r"C:\Data\Nite"
        os.makedirs(data_dir, exist_ok=True)
        print(f"üìÅ Using output directory: {data_dir}")
        
        if not os.path.exists(existing_file):
            print(f"‚ùå File not found: {existing_file}")
            return False
        
        print(f"üìÅ Processing SAP file: {existing_file}")
        
        # Read the file line by line to understand its structure
        with open(existing_file, 'r', encoding='utf-16') as f:
            lines = f.readlines()
        
        print(f"üìä File has {len(lines)} lines")
        
        # Find the data section (skip headers)
        data_start = None
        for i, line in enumerate(lines):
            if 'Centro' in line and 'Fe.Entrega' in line and 'Ruta' in line:
                data_start = i
                print(f"üìã Found header at line {i+1}")
                break
        
        if data_start is None:
            print("‚ùå Could not find data header")
            return False
        
        # Extract header
        header_line = lines[data_start].strip()
        headers = [col.strip() for col in header_line.split('\t')]
        print(f"üìã Found {len(headers)} columns: {headers[:5]}...")
        
        # Extract data rows
        data_rows = []
        for i in range(data_start + 1, len(lines)):
            line = lines[i].strip()
            if line and not line.startswith('16.09.2025'):  # Skip date headers
                row_data = line.split('\t')
                if len(row_data) >= len(headers):
                    data_rows.append(row_data[:len(headers)])
        
        print(f"üìä Found {len(data_rows)} data rows")
        
        # Create DataFrame
        df = pd.DataFrame(data_rows, columns=headers)
        
        # Clean up the DataFrame
        df.dropna(how='all', inplace=True)
        df.dropna(axis=1, how='all', inplace=True)
        df.reset_index(drop=True, inplace=True)
        
        print(f"‚úÖ DataFrame created with shape: {df.shape}")
        
        # Transform for Power BI
        df = transform_data_for_powerbi(df)
        
        # Save in multiple formats
        base_name = "REP_PLR_HOY"
        excel_path = os.path.join(data_dir, f"{base_name}_PowerBI.xlsx")
        csv_path = os.path.join(data_dir, f"{base_name}_PowerBI.csv")
        parquet_path = os.path.join(data_dir, f"{base_name}_PowerBI.parquet")
        
        save_powerbi_files(df, excel_path, csv_path, parquet_path)
        
        print("‚úÖ SAP file processed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error processing SAP file: {e}")
        return False


def transform_data_for_powerbi(df):
    """
    Transform and clean data for optimal Power BI compatibility
    """
    print("üîÑ Transforming data for Power BI compatibility...")
    
    # Create a copy to avoid modifying the original
    df_clean = df.copy()
    
    # Remove any rows that are completely empty or contain only whitespace
    df_clean = df_clean.dropna(how='all')
    df_clean = df_clean[~df_clean.astype(str).eq('').all(axis=1)]
    
    # Clean column names - remove extra spaces and special characters
    df_clean.columns = df_clean.columns.str.strip()
    df_clean.columns = df_clean.columns.str.replace(r'\s+', ' ', regex=True)
    
    # Convert date columns to proper datetime format
    date_columns = ['Fe.Entrega', 'Fecha Gu√≠a', 'Creado el']
    for col in date_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to datetime")
    
    # Convert time columns to proper time format
    time_columns = ['Hora Gu√≠a', 'Hora']
    for col in time_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_datetime(df_clean[col], format='%H:%M:%S', errors='coerce').dt.time
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to time")
    
    # Convert numeric columns
    numeric_columns = ['Cajas R.S.', 'Cajas F√≠sicas', 'Cajas Equiv.', 'Ruta', 'Entrega', 'Cliente']
    for col in numeric_columns:
        if col in df_clean.columns:
            try:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            except:
                print(f"‚ö†Ô∏è  Warning: Could not convert {col} to numeric")
    
    # Clean text columns - remove extra spaces
    text_columns = ['Centro', 'Nombre del Cliente', 'Estatus', 'Ruta Dist.', 'Cami√≥n', 
                   'Guia Entrega', 'Rol Alisto', 'Reprogramado', 'Ped. Clte', 'Ped. SAP',
                   'Tipo Ped.', 'Desc. Tipo', 'Origen', 'Tipo Ruta', 'Fuerza Ventas',
                   'Region', 'Zona Vtas', 'Desc Zona Vtas', 'Zona Superv', 'Desc Zona Superv',
                   'Agente Vtas', 'Nombre']
    
    for col in text_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str).str.strip()
            df_clean[col] = df_clean[col].replace('nan', '')
    
    # Add calculated columns useful for Power BI analysis
    if 'Fe.Entrega' in df_clean.columns:
        df_clean['A√±o'] = df_clean['Fe.Entrega'].dt.year
        df_clean['Mes'] = df_clean['Fe.Entrega'].dt.month
        df_clean['D√≠a'] = df_clean['Fe.Entrega'].dt.day
        df_clean['D√≠a_Semana'] = df_clean['Fe.Entrega'].dt.day_name()
        df_clean['Semana'] = df_clean['Fe.Entrega'].dt.isocalendar().week
    
    # Add performance indicators
    if 'Cajas R.S.' in df_clean.columns and 'Cajas F√≠sicas' in df_clean.columns:
        df_clean['Diferencia_Cajas'] = df_clean['Cajas F√≠sicas'] - df_clean['Cajas R.S.']
        df_clean['Porcentaje_Cumplimiento'] = (df_clean['Cajas F√≠sicas'] / df_clean['Cajas R.S.'] * 100).round(2)
    
    # Add status categories for better filtering in Power BI
    if 'Estatus' in df_clean.columns:
        df_clean['Estatus_Categoria'] = df_clean['Estatus'].apply(categorize_status)
    
    # Add route type categories
    if 'Tipo Ruta' in df_clean.columns:
        df_clean['Tipo_Ruta_Categoria'] = df_clean['Tipo Ruta'].apply(categorize_route_type)
    
    print(f"‚úÖ Data transformation completed. Final dataset shape: {df_clean.shape}")
    return df_clean


def categorize_status(status):
    """Categorize delivery status for better Power BI filtering"""
    if pd.isna(status):
        return 'Sin Estatus'
    
    status_str = str(status).strip()
    if status_str in ['5']:
        return 'Entregado'
    elif status_str in ['1', '2', '3']:
        return 'En Proceso'
    elif status_str in ['4']:
        return 'Pendiente'
    else:
        return 'Otro'


def categorize_route_type(route_type):
    """Categorize route type for better Power BI analysis"""
    if pd.isna(route_type):
        return 'Sin Tipo'
    
    route_str = str(route_type).strip().upper()
    if 'MODERNO' in route_str:
        return 'Moderno'
    elif 'TRADICIONAL' in route_str:
        return 'Tradicional'
    elif 'RURAL' in route_str:
        return 'Rural'
    else:
        return 'Otro'


def save_powerbi_files(df, excel_path, csv_path, parquet_path):
    """
    Save the dataframe in multiple formats optimized for Power BI
    """
    print("üíæ Saving files in Power BI compatible formats...")
    
    try:
        # Save as Excel with proper formatting
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='REP_PLR_Data', index=False)
            
            # Get the workbook and worksheet
            workbook = writer.book
            worksheet = writer.sheets['REP_PLR_Data']
            
            # Auto-adjust column widths
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"‚úÖ Excel file saved: {excel_path}")
        
        # Save as CSV with UTF-8 encoding (Power BI preferred)
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"‚úÖ CSV file saved: {csv_path}")
        
        # Save as Parquet for better performance in Power BI
        df.to_parquet(parquet_path, index=False)
        print(f"‚úÖ Parquet file saved: {parquet_path}")
        
        # Create a metadata file for Power BI
        create_powerbi_metadata(df, os.path.dirname(excel_path))
        
        print("\nüéâ All Power BI compatible files created successfully!")
        print("üìä Recommended file for Power BI: Use the .parquet file for best performance")
        print("üìã Use the .csv file if you need to import into other tools")
        print("üìà Use the .xlsx file for manual review and analysis")
        
    except Exception as e:
        print(f"‚ùå Error saving Power BI files: {e}")


def create_powerbi_metadata(df, output_path):
    """
    Create a metadata file with column descriptions for Power BI
    """
    try:
        metadata = {
            "dataset_info": {
                "name": "REP_PLR_HOY",
                "description": "Reporte de Planeamiento de Rutas - Datos del d√≠a",
                "created_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_records": len(df),
                "total_columns": len(df.columns)
            },
            "columns": {}
        }
        
        # Define column descriptions
        column_descriptions = {
            "Centro": "Centro de distribuci√≥n",
            "Fe.Entrega": "Fecha de entrega programada",
            "Ruta": "N√∫mero de ruta",
            "Entrega": "N√∫mero de entrega",
            "Cliente": "C√≥digo del cliente",
            "Nombre del Cliente": "Nombre del cliente",
            "Estatus": "Estatus de la entrega (5=Entregado)",
            "Verificado": "Indicador de verificaci√≥n",
            "Cajas R.S.": "Cajas requeridas seg√∫n sistema",
            "Cajas F√≠sicas": "Cajas f√≠sicas entregadas",
            "Cajas Equiv.": "Cajas equivalentes",
            "Ruta Dist.": "Ruta de distribuci√≥n",
            "Viaje": "N√∫mero de viaje",
            "Cami√≥n": "N√∫mero de cami√≥n",
            "Guia Entrega": "N√∫mero de gu√≠a de entrega",
            "Fecha Gu√≠a": "Fecha de la gu√≠a",
            "Hora Gu√≠a": "Hora de la gu√≠a",
            "Creado el": "Fecha de creaci√≥n del registro",
            "Hora": "Hora de creaci√≥n",
            "Rol Alisto": "Rol de alistamiento",
            "Reprogramado": "Indicador de reprogramaci√≥n",
            "Ped. Clte": "Pedido del cliente",
            "Ped. SAP": "Pedido en SAP",
            "Ped. Traslado": "Pedido de traslado",
            "Tipo Ped.": "Tipo de pedido",
            "Desc. Tipo": "Descripci√≥n del tipo",
            "Origen": "Origen del pedido",
            "Tipo Ruta": "Tipo de ruta",
            "Fuerza Ventas": "Fuerza de ventas",
            "Region": "Regi√≥n",
            "Zona Vtas": "Zona de ventas",
            "Desc Zona Vtas": "Descripci√≥n de zona de ventas",
            "Zona Superv": "Zona de supervisi√≥n",
            "Desc Zona Superv": "Descripci√≥n de zona de supervisi√≥n",
            "Agente Vtas": "C√≥digo del agente de ventas",
            "Nombre": "Nombre del agente de ventas",
            "A√±o": "A√±o de la entrega (calculado)",
            "Mes": "Mes de la entrega (calculado)",
            "D√≠a": "D√≠a de la entrega (calculado)",
            "D√≠a_Semana": "D√≠a de la semana (calculado)",
            "Semana": "Semana del a√±o (calculado)",
            "Diferencia_Cajas": "Diferencia entre cajas f√≠sicas y requeridas (calculado)",
            "Porcentaje_Cumplimiento": "Porcentaje de cumplimiento (calculado)",
            "Estatus_Categoria": "Categor√≠a del estatus (calculado)",
            "Tipo_Ruta_Categoria": "Categor√≠a del tipo de ruta (calculado)"
        }
        
        # Add column information
        for col in df.columns:
            metadata["columns"][col] = {
                "description": column_descriptions.get(col, "Sin descripci√≥n disponible"),
                "data_type": str(df[col].dtype),
                "null_count": int(df[col].isnull().sum()),
                "unique_values": int(df[col].nunique())
            }
        
        # Save metadata as JSON
        metadata_path = os.path.join(output_path, "REP_PLR_HOY_Metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Metadata file saved: {metadata_path}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not create metadata file: {e}")


if __name__ == "__main__":
    print("üöÄ Starting specialized SAP file processing...")
    print("=" * 60)
    
    success = process_sap_file()
    
    if success:
        print("\n" + "=" * 60)
        print("üéâ Process completed successfully!")
        print("üìÅ Check the 'data' folder for the Power BI compatible files:")
        print("   ‚Ä¢ REP_PLR_HOY_PowerBI.xlsx")
        print("   ‚Ä¢ REP_PLR_HOY_PowerBI.csv")
        print("   ‚Ä¢ REP_PLR_HOY_PowerBI.parquet")
        print("   ‚Ä¢ REP_PLR_HOY_Metadata.json")
    else:
        print("\n" + "=" * 60)
        print("‚ùå Process failed. Please check the error messages above.")
